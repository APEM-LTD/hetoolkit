# rename Survey.ID as rhs_survey_id
rhs_data <- rhs_data %>% dplyr::rename(rhs_survey_id = Survey.ID)
# Chunk 14: rhs_drops
# select Columns to keep
rhs_keep <- c("rhs_survey_id", "Hms.Poaching.Sub.Score", "Hms.Rsctned.Bnk.Bed.Sub.Score", "HMS.Score")
# select columns of interest
rhs_data <- rhs_data[ , (names(rhs_data) %in% rhs_keep)]
# Chunk 15: rhs_print
# view rhs_data
rhs_data
# Chunk 16: predict
# check substrate variables for presence of NAs
env_data %>%
select(BOULDERS_COBBLES, PEBBLES_GRAVEL, SAND, SILT_CLAY) %>%
summarise_all(funs(sum(is.na(.))))
# replace NAs, if required
env_data$BOULDERS_COBBLES[is.na(env_data$BOULDERS_COBBLES)] <- 0
# Chunk 17: predict_1
# run predictions
predict_data <- predict_indices(env_data = env_data)
# view predict_data
predict_data
# drop unwanted variables
keeps <- c("biol_site_id", "Season", "TL2_WHPT_ASPT_AbW_DistFam", "TL2_WHPT_NTAXA_AbW_DistFam", "TL3_LIFE_Fam_DistFam", "TL3_PSI_Fam")
predict_data <- predict_data[ , (names(predict_data) %in% keeps)]
# Chunk 18: join bio
# create Season and Year Columns
biol_data$Month <- month(biol_data$SAMPLE_DATE)
biol_data$Year <- year(biol_data$SAMPLE_DATE)
biol_data$Season <- ifelse((biol_data$Month >= 3) & (biol_data$Month <= 5), "Spring",
ifelse((biol_data$Month >= 6) & (biol_data$Month <= 8), "Summer",
ifelse((biol_data$Month >= 9) & (biol_data$Month <= 11), "Autumn", "Winter")))
# average out any replicate or duplicate biology samples
biol_data <- biol_data %>%
dplyr::group_by(biol_site_id, Year, Season) %>%
dplyr::summarise(WHPT_ASPT = mean(WHPT_ASPT),
WHPT_NTAXA = mean(WHPT_N_TAXA),
PSI_F = mean(PSI_FAMILY_SCORE),
LIFE_F = mean(LIFE_FAMILY_INDEX),
SAMPLE_DATE = mean(SAMPLE_DATE)) %>%
dplyr::ungroup()
# Chunk 19: join bio_1
# join expected and observed biology metric scores, by biol_site_id and Season
biol_all <- dplyr::left_join(biol_data, predict_data, by = c("biol_site_id", "Season"))
biol_all2 <- merge(biol_data, predict_data)
# Chunk 20: join bio_2
# join ENV data, by biol_site_id
biol_all <- dplyr::left_join(biol_all, env_data, by = "biol_site_id")
# Chunk 21: OE ratios
# WHPT-ASPT
biol_all <- biol_all %>%
mutate(
WHPT_ASPT_O = WHPT_ASPT,
WHPT_ASPT_E = TL2_WHPT_ASPT_AbW_DistFam,
WHPT_ASPT_OE = WHPT_ASPT_O / WHPT_ASPT_E,
WHPT_NTAXA_O = WHPT_NTAXA,
WHPT_NTAXA_E = TL2_WHPT_NTAXA_AbW_DistFam,
WHPT_NTAXA_OE = WHPT_NTAXA_O / WHPT_NTAXA_E,
LIFE_F_O = LIFE_F,
LIFE_F_E = TL3_LIFE_Fam_DistFam,
LIFE_F_OE = LIFE_F_O / LIFE_F_E,
PSI_O = PSI_F,
PSI_E = TL3_PSI_Fam,
PSI_OE = PSI_O / PSI_E
)
# Chunk 22: flow_source
# vew nrfa.hde
nrfa.hde
# Chunk 23: import flow
flow_data <- import_flow(sites = flowsites,
inputs = flowinputs,
start_date = "2010-01-01",
end_date = "2019-12-31",
dir = "data/wiski",
skip_num = 21,
col_order = c(1,2,3))
# Chunk 24: import flow_print
# view flow_data
flow_data
# Chunk 25: plot heatmap
# generate a heatmap of mean daily flows
a <- plot_heatmap(data = flow_data, x = "date", y = "flow_site_id", fill = "flow", dual = FALSE)
# view heatmap
gridExtra::grid.arrange(a[[1]])
# view table of completeness statistics for each site
a[[3]]
# uses flow_data, as produced using import_flow()
flow_data$month <- lubridate::month(flow_data$date)
flow_data$year <- lubridate::year(flow_data$date)
temp1 <- flow_data %>%
dplyr::group_by(month,year,flow_site_id) %>%
dplyr::summarise(across(.cols= "flow", list(
mean = ~mean(flow, na.rm = TRUE),
missing = ~length(which(is.na(flow))),
total = ~length(flow),
perc_missing = ~(length(which(is.na(flow)))/length(flow))*100), .names = "{.fn}"))
temp1$yy_mm <- paste(temp1$year, temp1$month, sep="_")
# heatmap of monthly mean flows.
a <- plot_heatmap(data = temp1, x = "yy_mm", y = "flow_site_id", fill = "mean", dual = FALSE)
# view heatmap
gridExtra::grid.arrange(a[[1]])
# heatmap of monthly percentage completeness of the daily flow data
a <- plot_heatmap(data = temp1, x = "yy_mm", y = "flow_site_id", fill = "perc_missing")
# view heatmap
gridExtra::grid.arrange(a[[1]])
# Chunk 26: calculate flow stats
# uses flow_data, as produced using import_flow()
# rename date and flow columns to those required by calc_flowstats
flow_data <- flow_data %>% dplyr::rename(Date = date)
flow_data <- flow_data %>% dplyr::rename(Flow = flow)
#  set any negative flow readings to NA
flow_data$Flow[flow_data$Flow <= 0] <- NA
# calculate flow statistics
flow_stats <- calc_flowstats(data = flow_data,
site_col = "flow_site_id",
date_col = "Date",
flow_col = "Flow")
# Chunk 27: calculate flow stats_print
# view time-varying flow statistics
flow_stats[[1]]
# Chunk 28: calculate flow stats_print2
# view long-term flow statistics
flow_stats[[2]]
# Chunk 29: biol_site_id
# rename SITE_ID to biol_site_id
#b iol_all <- biol_all %>% dplyr::rename(biol_site_id = SITE_ID)
# Chunk 30: join HE
# get output from calc_flowstats
flowstats_1 <- flow_stats[[1]]
# create two-column table mapping biology sites to flow sites
mapping <- master_data[, c("biol_site_id", "flow_site_id")]
# join flow statistics to biology data
join_data <- join_he(biol_data = biol_all,
flow_stats = flowstats_1,
mapping = mapping,
lag_vars = c("Q95z", "Q10z"),
LS1 = TRUE,
LS2 = TRUE)
# view join_data
join_data
# Chunk 31: join RHS
# create mapping
#rhs_survey_id <- as.character(rhssurveys)
#flow_site_id <- as.character(flowsites)
#mapping <- data.frame(flow_site_id, rhs_survey_id)
# create two-column table mapping biology sites to RHS surveys
mapping <- master_data[, c("biol_site_id", "rhs_survey_id")]
# Join RHS data to join_data
all_data <- dplyr::left_join(join_data, mapping, by = 'biol_site_id')
all_data <- dplyr::left_join(all_data, rhs_data, by = 'rhs_survey_id')
# Chunk 32: ggpairs1
GGally::ggpairs(all_data, columns=c("LIFE_F_OE", "WHPT_ASPT_OE", "WHPT_NTAXA_OE", "PSI_OE"),
upper = list(continuous = wrap("cor")),
diag = list(continuous = "densityDiag"),
lower = list(continuous = wrap("points")))
# Chunk 33: ggpairs2
GGally::ggpairs(all_data, columns=c("LIFE_F_OE", "Q95z", "Q10z", "Q95zLS1", "Q10zLS1"),
upper = list(continuous = wrap("cor")),
diag = list(continuous = "densityDiag"),
lower = list(continuous = wrap("points")))
# Chunk 34: sitepca
plot_sitepca(data = env_data, vars = c("ALTITUDE", "SLOPE", "WIDTH", "DEPTH", "BOULDERS_COBBLES", "PEBBLES_GRAVEL", "SILT_CLAY"), eigenvectors = TRUE, label_by = "biol_site_id")
# Chunk 35
plot_rngflows(data = all_data, flow_stats = c("Q95z", "Q10z"), biol_metric = "LIFE_F_OE", wrap_by = NULL, label = "Year")
plot_rngflows(data = all_data, flow_stats = c("Q95z", "Q10z"), biol_metric = "LIFE_F_OE", wrap_by = "biol_site_id", label = "Year")
# Chunk 36: hev1
plot_hev(data = subset(all_data, biol_site_id == "100582"),
date_col = "Year",
flow_stat = c("Q95z", "Q10z"),
biol_metric = c("LIFE_F_OE"),
clr_by = "Season")
# Chunk 37: hev2
sites_list <- unique(all_data$biol_site_id)
for(i in 1:length(sites_list)){
plot_data <- subset(all_data, biol_site_id == sites_list[i])
my_hev_plot <- plot_hev(data = plot_data,
date_col = "Year",
flow_stat = c("Q95z", "Q10z"),
biol_metric = c("LIFE_F_OE"),
clr_by = "Season")
ggsave(plot = my_hev_plot, device = "png", filename = paste0(sites_list[i],"_hevplot.png"))
}
autumn_data <- all_data %>% filter(Season == "Autumn")
autumn_data <- autumn_data %>% filter(is.na(LIFE_F_OE) == FALSE)
autumn_data <- autumn_data %>% filter(is.na(biol_site_id) == FALSE)
autumn_data <- autumn_data %>% filter(is.na(Q95z) == FALSE)
autumn_data <- autumn_data %>% filter(is.na(Q10z) == FALSE)
autumn_data <- autumn_data %>% filter(is.na(Q95zLS1) == FALSE)
autumn_data <- all_data %>% filter(Season == "Autumn")
View(autumn_data)
autumn_data <- autumn_data %>% filter(is.na(LIFE_F_OE) == FALSE)
autumn_data <- autumn_data %>% filter(is.na(biol_site_id) == FALSE)
autumn_data <- autumn_data %>% filter(is.na(Q95z) == FALSE)
autumn_data <- autumn_data %>% filter(is.na(Q10z) == FALSE)
autumn_data <- autumn_data %>% filter(is.na(Q95zLS1) == FALSE)
autumn_data <- all_data %>% filter(Season == "Autumn")
#filter data to eliminate NAs (necessary for cross-validation later on)
autumn_data <- autumn_data %>% filter(is.na(LIFE_F_OE) == FALSE)
autumn_data <- autumn_data %>% filter(is.na(biol_site_id) == FALSE)
autumn_data <- autumn_data %>% filter(is.na(Q95z) == FALSE)
autumn_data <- autumn_data %>% filter(is.na(Q10z) == FALSE)
View(autumn_data)
View(join_data)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2: load packages
rm(list = ls())
if(!require(pacman)) install.packages("pacman")
pacman::p_load(dplyr, lubridate, tictoc, readr, downloader, readxl, RCurl, writexl, tidyr, stringr, tibble, readr, htmlTable, tibbletime, devtools, roxygen2, downloader, shiny, plotly, RColorBrewer, ggnewscale, ggplot2, tidyverse, fasstr, lme4, sjPlot, sjmisc, RcppRoll, mgcv, viridis, gridExtra, naniar, purrr,ggfortify, plyr, GGally, forcats, visreg, formatR, sf, ggrepel, reshape)
# Load the package
devtools::load_all()
# Chunk 3: master data
# load master file
data("master_file")
# make all columns character vectors
master_file$biol_site_id <- as.character(master_file$biol_site_id)
master_file$rhs_survey_id <- as.character(master_file$rhs_survey_id)
# filter master file for selected sites of interest
master_data <- master_file %>% filter(biol_site_id %in% c("34310", "34343", "34352", "52504", "53819", "10708", "10752", "10784", "10707", "8314", "10992", "77599", "100582", "42760", "77216", "48210", "55824", "54017", "51122", "77216", "52828"))
# view data
master_data
# get site lists, for use with functions
biolsites <- master_data$biol_site_id
flowsites <- master_data$flow_site_id
flowinputs <- master_data$flow_input
rhssurveys <- master_data$rhs_survey_id
# Chunk 4: bio
# Import biology data from EDE
biol_data <- import_inv(sites = biolsites, start_date = "2010-01-01", end_date = "2020-12-31")
# Chunk 5: bio_table
# view biol_data
biol_data
# Chunk 6: biol_drops
# bind 2 biology data sets - one from EDE and one local file
# drop any unwanted variables/columns from the EDE download file
drops_bio <- c("SAMPLE_VERSION", "REPLICATE_CODE", "SAMPLE_TYPE", "SAMPLE_METHOD", "ANALYSIS_TYPE", "ANALYSIS_METHOD", "IS_THIRD_PARTY_DATA", "WATERBODY_TYPE")
# drop unwanted variables
biol_data2 <- biol_data[ , !(names(biol_data) %in% drops_bio)]
# read in additional biology data in csv format
biol_data_excel <- read.csv("data/biol_data_join.csv")
# format columns
biol_data_excel <- biol_data_excel %>% dplyr::mutate(biol_site_id = as.character(biol_site_id))
# convert to tibble format
biol_data_excel <- as.tibble(biol_data_excel)
# bind datasets
biol_data_final <- rbind(biol_data2, biol_data_excel)
# Chunk 7: env
# Import biology data from EDE
env_data <- import_env(sites = biolsites)
# Chunk 8: env_table
# view env_data
env_data
# Chunk 9: get_data
# get shapefile for EA areas
download('https://environment.data.gov.uk/UserDownloads/interactive/d73368b35bb74163a384d3d29e479c0d75644/EA_AdminBoundEAandNEpublicFaceAreas_SHP_Full.zip',
destfile = 'EA_AdminBoundEAandNEpublicFaceAreas_SHP_Full.zip')
# get Wales coastline, just because map looks silly without it
download('https://borders.ukdataservice.ac.uk/ukborders/easy_download/prebuilt/shape/Wales_ol_2001.zip',
destfile = 'Wales_ol_2001.zip')
# load in the shapefiles to R objects and remove seaward parts
EA.areas <- read_sf('/vsizip/EA_AdminBoundEAandNEpublicFaceAreas_SHP_Full.zip/data/Administrative_Boundaries_Environment_Agency_and_Natural_England_Public_Face_Areas.shp')
wales <- read_sf('/vsizip/Wales_ol_2001.zip')
EA.areas.land <- EA.areas %>% dplyr::filter(seaward == 'No')
# Chunk 10: processing
# translate NGRs to full easting / northing
temp.eastnorths <- osg_parse(env_data$NGR_10_FIG, coord_system = "BNG") %>% as_tibble()
# make an sf point object to plot with geom_sf but retain the easting and northing column required by geom_text_repel
env_sf <- st_as_sf(bind_cols(temp.eastnorths, env_data), coords = c("easting", "northing"), crs = 27700, remove = FALSE)
# Chunk 11: map
basemap <- ggplot() + geom_sf(mapping = aes(), data = EA.areas.land) +
geom_sf(mapping = aes(), data = wales)
basemap + geom_sf(data = env_sf, col = "blue", size = 1.5) +
geom_text_repel(data = env_sf, aes(x=easting, y=northing, label = biol_site_id), col = "blue") +
coord_sf(datum = sf::st_crs(27700))
# Chunk 12: env_drops
# bind 2 env data sets - one from EDE and one local file
# drop any unwanted variables/columns from the download file
drops_env <- c("AGENCY_AREA", "CATCHMENT", "WATERBODY_TYPE", "WATERBODY_TYPE_DESCRIPTION", "SITE_VERSION", "NGR_10_FIG", "FULL_EASTING", "FULL_NORTHING", "BASE_DATA_DATE", "MIN_SAMPLE_DATE", "MAX_SAMPLE_DATE", "COUNT_OF_SAMPLES", "ECN_SITE", "INV", "ECN_SITE_INV")
# drop unwanted variables
env_data <- env_data[ , !(names(env_data) %in% drops_env)]
# read-in environmental data in excel format
env_data_excel <- read_excel("data/Env_Additonal_Sites.xlsx")
# format columns
env_data_excel <- env_data_excel %>% dplyr::mutate(
biol_site_id = as.character(biol_site_id),
WATER_BODY = as.character(WATER_BODY),
NGR_PREFIX = as.character(NGR_PREFIX),
EASTING = as.character(EASTING),
NORTHING = as.character(NORTHING),
WFD_WATERBODY_ID = as.character(WFD_WATERBODY_ID))
# convert to tibble format
env_data_excel <- as.tibble(env_data_excel)
# join datasets
env_data_final <- rbind(env_data, env_data_excel)
# Chunk 13: rhs
# import RHS data from Open Data
rhs_data <- import_rhs(surveys =  rhssurveys)
# rename Survey.ID as rhs_survey_id
rhs_data <- rhs_data %>% dplyr::rename(rhs_survey_id = Survey.ID)
# Chunk 14: rhs_drops
# select Columns to keep
rhs_keep <- c("rhs_survey_id", "Hms.Poaching.Sub.Score", "Hms.Rsctned.Bnk.Bed.Sub.Score", "HMS.Score")
# select columns of interest
rhs_data <- rhs_data[ , (names(rhs_data) %in% rhs_keep)]
# Chunk 15: rhs_print
# view rhs_data
rhs_data
# Chunk 16: predict
# check substrate variables for presence of NAs
env_data %>%
select(BOULDERS_COBBLES, PEBBLES_GRAVEL, SAND, SILT_CLAY) %>%
summarise_all(funs(sum(is.na(.))))
# replace NAs, if required
env_data$BOULDERS_COBBLES[is.na(env_data$BOULDERS_COBBLES)] <- 0
# Chunk 17: predict_1
# run predictions
predict_data <- predict_indices(env_data = env_data)
# view predict_data
predict_data
# drop unwanted variables
keeps <- c("biol_site_id", "Season", "TL2_WHPT_ASPT_AbW_DistFam", "TL2_WHPT_NTAXA_AbW_DistFam", "TL3_LIFE_Fam_DistFam", "TL3_PSI_Fam")
predict_data <- predict_data[ , (names(predict_data) %in% keeps)]
# Chunk 18: join bio
# create Season and Year Columns
biol_data$Month <- month(biol_data$SAMPLE_DATE)
biol_data$Year <- year(biol_data$SAMPLE_DATE)
biol_data$Season <- ifelse((biol_data$Month >= 3) & (biol_data$Month <= 5), "Spring",
ifelse((biol_data$Month >= 6) & (biol_data$Month <= 8), "Summer",
ifelse((biol_data$Month >= 9) & (biol_data$Month <= 11), "Autumn", "Winter")))
# average out any replicate or duplicate biology samples
biol_data <- biol_data %>%
dplyr::group_by(biol_site_id, Year, Season) %>%
dplyr::summarise(WHPT_ASPT = mean(WHPT_ASPT),
WHPT_NTAXA = mean(WHPT_N_TAXA),
PSI_F = mean(PSI_FAMILY_SCORE),
LIFE_F = mean(LIFE_FAMILY_INDEX),
SAMPLE_DATE = mean(SAMPLE_DATE)) %>%
dplyr::ungroup()
# Chunk 19: join bio_1
# join expected and observed biology metric scores, by biol_site_id and Season
biol_all <- dplyr::left_join(biol_data, predict_data, by = c("biol_site_id", "Season"))
biol_all2 <- merge(biol_data, predict_data)
# Chunk 20: join bio_2
# join ENV data, by biol_site_id
biol_all <- dplyr::left_join(biol_all, env_data, by = "biol_site_id")
# Chunk 21: OE ratios
# WHPT-ASPT
biol_all <- biol_all %>%
mutate(
WHPT_ASPT_O = WHPT_ASPT,
WHPT_ASPT_E = TL2_WHPT_ASPT_AbW_DistFam,
WHPT_ASPT_OE = WHPT_ASPT_O / WHPT_ASPT_E,
WHPT_NTAXA_O = WHPT_NTAXA,
WHPT_NTAXA_E = TL2_WHPT_NTAXA_AbW_DistFam,
WHPT_NTAXA_OE = WHPT_NTAXA_O / WHPT_NTAXA_E,
LIFE_F_O = LIFE_F,
LIFE_F_E = TL3_LIFE_Fam_DistFam,
LIFE_F_OE = LIFE_F_O / LIFE_F_E,
PSI_O = PSI_F,
PSI_E = TL3_PSI_Fam,
PSI_OE = PSI_O / PSI_E
)
# Chunk 22: flow_source
# vew nrfa.hde
nrfa.hde
# Chunk 23: import flow
flow_data <- import_flow(sites = flowsites,
inputs = flowinputs,
start_date = "2010-01-01",
end_date = "2019-12-31",
dir = "data/wiski",
skip_num = 21,
col_order = c(1,2,3))
# Chunk 24: import flow_print
# view flow_data
flow_data
# Chunk 25: plot heatmap
# generate a heatmap of mean daily flows
a <- plot_heatmap(data = flow_data, x = "date", y = "flow_site_id", fill = "flow", dual = FALSE)
# view heatmap
gridExtra::grid.arrange(a[[1]])
# view table of completeness statistics for each site
a[[3]]
# uses flow_data, as produced using import_flow()
flow_data$month <- lubridate::month(flow_data$date)
flow_data$year <- lubridate::year(flow_data$date)
temp1 <- flow_data %>%
dplyr::group_by(month,year,flow_site_id) %>%
dplyr::summarise(across(.cols= "flow", list(
mean = ~mean(flow, na.rm = TRUE),
missing = ~length(which(is.na(flow))),
total = ~length(flow),
perc_missing = ~(length(which(is.na(flow)))/length(flow))*100), .names = "{.fn}"))
temp1$yy_mm <- paste(temp1$year, temp1$month, sep="_")
# heatmap of monthly mean flows.
a <- plot_heatmap(data = temp1, x = "yy_mm", y = "flow_site_id", fill = "mean", dual = FALSE)
# view heatmap
gridExtra::grid.arrange(a[[1]])
# heatmap of monthly percentage completeness of the daily flow data
a <- plot_heatmap(data = temp1, x = "yy_mm", y = "flow_site_id", fill = "perc_missing")
# view heatmap
gridExtra::grid.arrange(a[[1]])
# Chunk 26: calculate flow stats
# uses flow_data, as produced using import_flow()
# rename date and flow columns to those required by calc_flowstats
flow_data <- flow_data %>% dplyr::rename(Date = date)
flow_data <- flow_data %>% dplyr::rename(Flow = flow)
#  set any negative flow readings to NA
flow_data$Flow[flow_data$Flow <= 0] <- NA
# calculate flow statistics
flow_stats <- calc_flowstats(data = flow_data,
site_col = "flow_site_id",
date_col = "Date",
flow_col = "Flow")
# Chunk 27: calculate flow stats_print
# view time-varying flow statistics
flow_stats[[1]]
# Chunk 28: calculate flow stats_print2
# view long-term flow statistics
flow_stats[[2]]
# Chunk 29: biol_site_id
# rename SITE_ID to biol_site_id
#b iol_all <- biol_all %>% dplyr::rename(biol_site_id = SITE_ID)
# Chunk 30: join HE
# get output from calc_flowstats
flowstats_1 <- flow_stats[[1]]
# create two-column table mapping biology sites to flow sites
mapping <- master_data[, c("biol_site_id", "flow_site_id")]
# join flow statistics to biology data
join_data <- join_he(biol_data = biol_all,
flow_stats = flowstats_1,
mapping = mapping,
lag_vars = c("Q95z", "Q10z"),
LS1 = TRUE,
LS2 = TRUE)
# view join_data
join_data
# Chunk 31: join RHS
# create mapping
#rhs_survey_id <- as.character(rhssurveys)
#flow_site_id <- as.character(flowsites)
#mapping <- data.frame(flow_site_id, rhs_survey_id)
# create two-column table mapping biology sites to RHS surveys
mapping <- master_data[, c("biol_site_id", "rhs_survey_id")]
# Join RHS data to join_data
all_data <- dplyr::left_join(join_data, mapping, by = 'biol_site_id')
all_data <- dplyr::left_join(all_data, rhs_data, by = 'rhs_survey_id')
# Chunk 32: ggpairs1
GGally::ggpairs(all_data, columns=c("LIFE_F_OE", "WHPT_ASPT_OE", "WHPT_NTAXA_OE", "PSI_OE"),
upper = list(continuous = wrap("cor")),
diag = list(continuous = "densityDiag"),
lower = list(continuous = wrap("points")))
# Chunk 33: ggpairs2
GGally::ggpairs(all_data, columns=c("LIFE_F_OE", "Q95z", "Q10z", "Q95zLS1", "Q10zLS1"),
upper = list(continuous = wrap("cor")),
diag = list(continuous = "densityDiag"),
lower = list(continuous = wrap("points")))
# Chunk 34: sitepca
plot_sitepca(data = env_data, vars = c("ALTITUDE", "SLOPE", "WIDTH", "DEPTH", "BOULDERS_COBBLES", "PEBBLES_GRAVEL", "SILT_CLAY"), eigenvectors = TRUE, label_by = "biol_site_id")
# Chunk 35
plot_rngflows(data = all_data, flow_stats = c("Q95z", "Q10z"), biol_metric = "LIFE_F_OE", wrap_by = NULL, label = "Year")
plot_rngflows(data = all_data, flow_stats = c("Q95z", "Q10z"), biol_metric = "LIFE_F_OE", wrap_by = "biol_site_id", label = "Year")
# Chunk 36: hev1
plot_hev(data = subset(all_data, biol_site_id == "100582"),
date_col = "Year",
flow_stat = c("Q95z", "Q10z"),
biol_metric = c("LIFE_F_OE"),
clr_by = "Season")
# Chunk 37: hev2
sites_list <- unique(all_data$biol_site_id)
for(i in 1:length(sites_list)){
plot_data <- subset(all_data, biol_site_id == sites_list[i])
my_hev_plot <- plot_hev(data = plot_data,
date_col = "Year",
flow_stat = c("Q95z", "Q10z"),
biol_metric = c("LIFE_F_OE"),
clr_by = "Season")
ggsave(plot = my_hev_plot, device = "png", filename = paste0(sites_list[i],"_hevplot.png"))
}
# filter data by season
autumn_data <- all_data %>% filter(Season == "Autumn")
#filter data to eliminate NAs (necessary for cross-validation later on)
autumn_data <- autumn_data %>% filter(is.na(LIFE_F_OE) == FALSE)
autumn_data <- autumn_data %>% filter(is.na(biol_site_id) == FALSE)
autumn_data <- autumn_data %>% filter(is.na(Q95z) == FALSE)
autumn_data <- autumn_data %>% filter(is.na(Q10z) == FALSE)
autumn_data <- autumn_data %>% filter(is.na(Q95zLS1) == FALSE)
model_full <- lmer(LIFE_F_OE ~ Q95z + Q10z + Q95zLS1 + (Q95z | biol_site_id), data = autumn_data, REML = FALSE)
summary(model_full)
install.packages("merTools")
library(merTools)
install.packages("arm")
library(merTools)
library(arm)
install.packages("Formula")
library(arm)
library(Formula)
install.packages("base64enc")
library(arm)
install.packages("latticeExtra")
library(arm)
install.packages("coda")
library(arm)
library(merTools)
install.packages("blme")
library(merTools)
install.packages("broom.mixed")
library(merTools)
install.packages("foreach")
library(merTools)
install.packages("iterators")
library(merTools)
